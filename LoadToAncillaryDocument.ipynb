{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import getpass\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "import numpy as np\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigNotFoundError(Exception):\n",
    "    \"\"\"Custom exception for configuration not found errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ConnectionError(Exception):\n",
    "    \"\"\"Custom exception for connection errors.\"\"\"\n",
    "\n",
    "class SSMSConnnectionManager:\n",
    "    def __init__(self, config, password):\n",
    "        self.config = config\n",
    "        self.password = password\n",
    "        self.driver = None \n",
    "        self.server = None\n",
    "        self.port = None\n",
    "        self.database = None\n",
    "        self.username = None\n",
    "        self.engine = None\n",
    "        self.pyodbc_connect = None\n",
    "        self.connecting_string = None\n",
    "\n",
    "    def set_connection_config(self):\n",
    "        if self.config['Environment']:\n",
    "            env = self.config['Environment']\n",
    "        else: \n",
    "            raise Exception('Environment configuration not found')\n",
    "\n",
    "        for key, value in self.config.items():\n",
    "            try:\n",
    "                if 'SSMSConnectionManager' in key:\n",
    "                    server_name = env+'Server'\n",
    "                    self.driver = value['Driver']\n",
    "                    self.server = value[server_name]\n",
    "                    self.port = value['Port']\n",
    "                    self.database = value['Database']\n",
    "                    self.username = value['Username']\n",
    "            except Exception as e:\n",
    "                print(f\"SSMSConnectionManager configuration failure. The following configuration has not been set correctly: {e}\")\n",
    "\n",
    "    def connect(self):\n",
    "        try:   \n",
    "            self.connecting_string = f\"Driver={self.driver};Server={self.server},{self.port};Database={self.database};UID={self.username};PWD={self.password};&autocommit=true\"\n",
    "            connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\":self.connecting_string})\n",
    "            engine = create_engine(connection_url, use_setinputsizes=False, echo=False)\n",
    "            self.pyodbc_connect = pyodbc.connect(self.connecting_string)\n",
    "            self.engine = engine.connect()\n",
    "            print(\"Connected to SSMS successfully.\")\n",
    "            return(self.pyodbc_connect)\n",
    "        except KeyError as e:\n",
    "            raise ConfigNotFoundError(f\"Missing required configuration key: {e}\")\n",
    "        except pyodbc.Error as e:\n",
    "                raise ConnectionError(f\"Error connecting to SSMS: {e}\")\n",
    "\n",
    "    def query_table(self, query):\n",
    "        if not self.engine:\n",
    "            print(\"No active connection.\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_sql(query, con=self.engine)\n",
    "            print(\"Query executed successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def insert_into_sql(self, df, tablename, columns):\n",
    "        conn = self.pyodbc_connect\n",
    "        cursor = self.pyodbc_connect.cursor()\n",
    "\n",
    "        # Create the SQL insert statement dynamically\n",
    "        placeholders = ', '.join(['?'] * len(columns))\n",
    "        columns_str = ', '.join(columns)\n",
    "        sql = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Prepare the data for insertion\n",
    "        data = df[columns].values.tolist()\n",
    "\n",
    "        if not self.pyodbc_connect:\n",
    "            print(\"No active connection.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Execute the insert statement\n",
    "            cursor.executemany(sql, data)\n",
    "\n",
    "            # Commit the transaction\n",
    "            self.pyodbc_connect.commit()\n",
    "\n",
    "            # Close the connection\n",
    "            #cursor.close()\n",
    "            #self.pyodbc_connect.close()\n",
    "            print(\"Data imported successfully.\")\n",
    "        except (Exception, pyodbc.DatabaseError) as error:\n",
    "            # Rollback the transaction on failure\n",
    "            self.pyodbc_connect.rollback()\n",
    "            print(f\"Error importing data into {tablename}: {error}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            self.pyodbc_connect.close()\n",
    "\n",
    "    def update_table_with_dataframe(self, df, table_name, key_columns, update_columns):\n",
    "        \"\"\"\n",
    "        Update the table with the dataframe values.\n",
    "        :param conn: Database connection object\n",
    "        :param df: pandas DataFrame with the data to update\n",
    "        :param table_name: Name of the table to update\n",
    "        :param key_columns: List of column names to use as keys for the update\n",
    "        :param update_columns: List of column names to update\n",
    "        \"\"\"\n",
    "        conn = self.pyodbc_connect\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            for index, row in df.iterrows():\n",
    "                set_clause = ', '.join([f\"{col} = ?\" for col in update_columns])\n",
    "                where_clause = ' AND '.join([f\"{col} = ?\" for col in key_columns])\n",
    "                sql_query = f\"UPDATE {table_name} SET {set_clause} WHERE {where_clause}\"\n",
    "                parameters = [row[col] for col in update_columns] + [row[col] for col in key_columns]\n",
    "                cursor.execute(sql_query, parameters)\n",
    "            conn.commit()\n",
    "        except (Exception, pyodbc.DatabaseError) as error:\n",
    "            print(f\"Error: {error}\")\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            self.pyodbc_connect.close()\n",
    "\n",
    "    def close(self):\n",
    "        if self.engine:\n",
    "            self.engine.close()\n",
    "            print(\"Connection to SSMS closed.\")\n",
    "        else:\n",
    "            print(\"No active connection to close.\")\n",
    "\n",
    "    def run_connection_manager(self):\n",
    "        try:\n",
    "            self.set_connection_config()\n",
    "            self.connect()\n",
    "            return(self.pyodbc_connect)\n",
    "        except (ConfigNotFoundError, ConnectionError) as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class carrierConfig:\n",
    "    def __init__(self, config_file_path):\n",
    "       self.config_file_path = config_file_path\n",
    "       self.config = self.read_config()\n",
    "\n",
    "    def read_config(self):\n",
    "        try:\n",
    "            with open(self.config_file_path, 'r') as file:\n",
    "                config = json.load(file)\n",
    "            return(config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading config file: {e}\")\n",
    "            return(None)\n",
    "\n",
    "    def get_carrier_config(self, config=None):\n",
    "        if config is None:\n",
    "            config = self.config\n",
    "\n",
    "        config_list = [(obj['carrierId'], obj['carrierName'], obj[\"carrierFileType\"], f\"{config['FileDetails']['mainPath']+obj['carrierName']}\\\\\", obj[\"delimiter\"]) for obj in config.get('CarrierDetials')]\n",
    "\n",
    "        carrier_config_worklist = []\n",
    "\n",
    "        def create_carrier_filename_worklist (worklist):\n",
    "            for details in worklist:\n",
    "                carrier_id = details[0]\n",
    "                carrier_file_type = details[2]\n",
    "                if os.path.isdir(details[3]):\n",
    "                    for fileName in os.listdir(details[3]):\n",
    "                        if carrier_file_type in fileName:\n",
    "                            carrier_config_worklist.append((details[0],details[1],details[3]+fileName,details[4]))\n",
    "                        else:\n",
    "                            continue\n",
    "                            #print(f'No file listed in current directory: {details[3]}')\n",
    "                else:\n",
    "                    print(f\"The directory '{details[3]}' does not exist.\")\n",
    "\n",
    "        create_carrier_filename_worklist(config_list)\n",
    "        return carrier_config_worklist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrierLoader:\n",
    "    def __init__(self, carrier_worklist):\n",
    "        self.carrier_worklist = carrier_worklist\n",
    "        self.standard_headers = ['Carrier Name','Brand Name','Plan Name','State','Document Type','Language','Effective Date','Document URL']\n",
    "        self.carrierWorklist = None\n",
    "        self.read_method = None\n",
    "        self.df = None\n",
    "        self.log_df = None\n",
    "\n",
    "    def load_file(self):\n",
    "        dataframe = pd.DataFrame()\n",
    "        files_with_issues = []\n",
    "        try:\n",
    "            # loop through files and check if columns in file matches standard_headers and load to dataframe if matched. If not create a dataframe output errors to logging. \n",
    "            for carrierWorklist in self.carrier_worklist:\n",
    "                file_path = carrierWorklist[2]\n",
    "                delimiter_len = len(carrierWorklist[3])\n",
    "                # Read the Excel file\n",
    "                if file_path.split('.')[-1] in ['xlsx','xls'] and delimiter_len == 0:\n",
    "                    df = pd.read_excel(file_path, dtype={'Document URL': str})\n",
    "                    \n",
    "                    # Get the headers of the DataFrame\n",
    "                    file_headers = df.columns.tolist()\n",
    "                    \n",
    "                    # Check if the file headers match the standard headers\n",
    "                    if all(column in file_headers for column in self.standard_headers):\n",
    "                        print(f\"Headers in {file_path} match the standard headers.\")\n",
    "                        # Add new column in df\n",
    "                        df['CarrierId'] = carrierWorklist[0]\n",
    "                        df['EffectiveEndDate'] = ''\n",
    "                        df['LoadDate'] = pd.Timestamp.now()\n",
    "                        df['fileName'] =  carrierWorklist[2] #carrierWorklist[2].split('\\\\')[-1]\n",
    "                        df['FileLoadIndicator'] = 'Y'\n",
    "                        df.rename(columns={'Carrier Name':'CarrierName', 'Brand Name':'BrandName', 'Document Type':'DocumentType', 'Effective Date':'EffectiveBeginDate', 'Document URL':'DocumentUrl', 'Plan Name':'PlanName', 'StateCd':'State'},inplace=True)\n",
    "                        df = df[['CarrierId','CarrierName','BrandName','PlanName','State','DocumentType','Language','EffectiveBeginDate','EffectiveEndDate','DocumentUrl','LoadDate','fileName','FileLoadIndicator']]\n",
    "                        dataframe = pd.concat([dataframe, df], ignore_index=True)\n",
    "                    else:\n",
    "                        print(f\"Headers in {file_path} do NOT match the standard headers.\")\n",
    "                        files_with_issues.append((f\"{file_path}\", \"Headers do Not match standard template.\"))\n",
    "                # Read the Csv file without delimiter\n",
    "                elif file_path.split('.')[-1] in ['csv','txt','text'] and delimiter_len == 0:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Get the headers of the DataFrame\n",
    "                    file_headers = df.columns.tolist()\n",
    "                    \n",
    "                    # Check if the file headers match the standard headers\n",
    "                    if all(column in file_headers for column in self.standard_headers):\n",
    "                        print(f\"Headers in {file_path} match the standard headers.\")\n",
    "                        # Add new column in df\n",
    "                        df['CarrierId'] = carrierWorklist[0]\n",
    "                        df['EffectiveEndDate'] = ''\n",
    "                        df['LoadDate'] = pd.Timestamp.now()\n",
    "                        df['fileName'] =  carrierWorklist[2] #carrierWorklist[2].split('\\\\')[-1]\n",
    "                        df['FileLoadIndicator'] = 'Y'\n",
    "                        df.rename(columns={'Carrier Name':'CarrierName', 'Brand Name':'BrandName', 'Document Type':'DocumentType', 'Effective Date':'EffectiveBeginDate', 'Document URL':'DocumentUrl', 'Plan Name':'PlanName', 'StateCd':'State'},inplace=True)\n",
    "                        df = df[['CarrierId','CarrierName','BrandName','PlanName','State','DocumentType','Language','EffectiveBeginDate','EffectiveEndDate','DocumentUrl','LoadDate','fileName','FileLoadIndicator']]\n",
    "                        dataframe = pd.concat([dataframe, df], ignore_index=True)\n",
    "                        #dataframe.append(df)\n",
    "                    else:\n",
    "                        print(f\"Headers in {file_path} do NOT match the standard headers.\")\n",
    "                        files_with_issues.append((f\"{file_path}\", \"Headers do Not match standard template.\"))\n",
    "                # Read the Csv file with delimiter\n",
    "                elif file_path.split('.')[-1] in ['csv','txt','text'] and delimiter_len > 0:\n",
    "                    df = pd.read_csv(file_path, delimiter= carrierWorklist[3])\n",
    "                    \n",
    "                    # Get the headers of the DataFrame\n",
    "                    file_headers = df.columns.tolist()\n",
    "                    \n",
    "                    # Check if the file headers match the standard headers\n",
    "                    if all(column in file_headers for column in self.standard_headers):\n",
    "                        print(f\"Headers in {file_path} match the standard headers.\")\n",
    "                        # Add new column in df\n",
    "                        df['CarrierId'] = carrierWorklist[0]\n",
    "                        df['EffectiveEndDate'] = ''\n",
    "                        df['LoadDate'] = pd.Timestamp.now()\n",
    "                        df['fileName'] =  carrierWorklist[2] #carrierWorklist[2].split('\\\\')[-1]\n",
    "                        df['FileLoadIndicator'] = 'Y'\n",
    "                        df.rename(columns={'Carrier Name':'CarrierName', 'Brand Name':'BrandName', 'Document Type':'DocumentType', 'Effective Date':'EffectiveBeginDate', 'Document URL':'DocumentUrl', 'Plan Name':'PlanName', 'StateCd':'State'},inplace=True)\n",
    "                        df = df[['CarrierId','CarrierName','BrandName','PlanName','State','DocumentType','Language','EffectiveBeginDate','EffectiveEndDate','DocumentUrl','LoadDate','fileName','FileLoadIndicator']]\n",
    "                        dataframe = pd.concat([dataframe, df], ignore_index=True)\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Headers in {file_path} do NOT match the standard headers.\")\n",
    "                        failed_desc_list = [{'SourceFile': f\"{file_path}\", 'ErrorDescription':\"Headers don't match standard template.\"}]\n",
    "                        files_with_issues.append((f\"{file_path}\", \"Headers do Not match standard template.\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file '{carrierWorklist[2]}': {e}\")     \n",
    "            \n",
    "        self.df = dataframe.drop_duplicates()\n",
    "        self.log_df = pd.DataFrame(files_with_issues, columns=['SourceFile','ErrorDescription'])\n",
    "        return(self.df, self.log_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class validations:\n",
    "    def __init__(self,main_df,validation_restuls_df,column_dups,columns_to_check):\n",
    "        self.main_df = main_df\n",
    "        self.validation_restuls_df = validation_restuls_df\n",
    "        self.error_logging_dup_df = None\n",
    "        self.error_logging_column_df = None\n",
    "        self.column_dups = column_dups\n",
    "        self.columns_to_check = columns_to_check\n",
    "\n",
    "    def validate_no_duplicates(self):\n",
    "        # Check for duplicates based on self.column_dups\n",
    "        duplicates = self.main_df.duplicated(subset=self.column_dups, keep=False)\n",
    "        # Display rows that are duplicates based on self.column_dups\n",
    "        results = self.main_df[duplicates]\n",
    "        duplicate_df = pd.DataFrame()\n",
    "\n",
    "        if len(results['fileName'].unique().tolist()) == 0:\n",
    "            print(\"Duplicate Validations: Passed\")\n",
    "        else:\n",
    "            for fileName in results['fileName'].unique().tolist():\n",
    "                error_logging_dup_df = pd.DataFrame({'fileName':[f'{fileName}'], 'ErrorDescription':[\"Please check source file for duplicates.\"]})\n",
    "                self.error_logging_dup_df =  pd.concat([duplicate_df, error_logging_dup_df], ignore_index=True)\n",
    "            return (duplicate_df)\n",
    "\n",
    "    def validate_no_blanks_or_nulls_in_column(self):\n",
    "        # Identify rows where 'ProductStateId' is null or blank, if so log errors becasue data was not found to be mapped on. \n",
    "        column_df = pd.DataFrame()\n",
    "        for column in self.columns_to_check:\n",
    "            results = self.main_df[self.main_df[column].isnull() | (self.main_df[column] == '')]\n",
    "\n",
    "            if len(results['fileName'].unique().tolist()) != 0:\n",
    "                for fileName in results['fileName'].unique().tolist():\n",
    "                    column_error_df = pd.DataFrame({'fileName':[f'{fileName}'], 'ErrorDescription':[f\"Column not populated '{column}'\"]})\n",
    "                    self.error_logging_column_df =  pd.concat([column_df, column_error_df], ignore_index=True)\n",
    "                    #print(f\"Please check source file '{fileName}', value not populated for the following column '{column}'.\")\n",
    "            else:\n",
    "                print(f\"Check column '{column}' for blanks or null values: Passed\")\n",
    "        return (column_df)\n",
    "    \n",
    "    def combine_values_dataframe(self):\n",
    "        # consolidates all values from data that match on as specific column & create new column 'FileLoadIndicator' = 'N' to for files not to load\n",
    "        #if self.error_logging_dup_df is not None or self.error_logging_column_df is not None:\n",
    "        if  self.error_logging_dup_df is not None and self.error_logging_column_df is not None:\n",
    "            self.validation_restuls_df = pd.concat([self.error_logging_dup_df, self.error_logging_column_df], ignore_index=True)\n",
    "            self.validation_restuls_df = self.validation_restuls_df.groupby('fileName')['ErrorDescription'].agg(lambda x: '; '.join(x)).reset_index()\n",
    "            self.validation_restuls_df['FileLoadIndicator'] = 'N'\n",
    "            print(\"Valdations completed: Failed, please check logs\")\n",
    "        elif self.error_logging_dup_df is not None and self.error_logging_column_df is None:\n",
    "            self.validation_restuls_df = self.error_logging_dup_df\n",
    "            self.validation_restuls_df = self.validation_restuls_df.groupby('fileName')['ErrorDescription'].agg(lambda x: '; '.join(x)).reset_index()\n",
    "            self.validation_restuls_df['FileLoadIndicator'] = 'N'\n",
    "            print(\"Valdations completed: Failed, please check logs\")\n",
    "        elif self.error_logging_dup_df is None and self.error_logging_column_df is not None:\n",
    "            self.validation_restuls_df = self.error_logging_column_df\n",
    "            self.validation_restuls_df = self.validation_restuls_df.groupby('fileName')['ErrorDescription'].agg(lambda x: '; '.join(x)).reset_index()\n",
    "            self.validation_restuls_df['FileLoadIndicator'] = 'N'\n",
    "            print(\"Valdations completed: Failed, please check logs\")\n",
    "        else:\n",
    "            print(\"Validations completed.....\")\n",
    "            \n",
    "        return(self.validation_restuls_df)\n",
    "\n",
    "    def run_all_validations(self):\n",
    "        self.validate_no_duplicates()\n",
    "        self.validate_no_blanks_or_nulls_in_column()\n",
    "        self.combine_values_dataframe()\n",
    "        return(self.validation_restuls_df)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers in D:\\Data\\Ancillary\\HI\\Documents\\Cigna\\Cigna HIP Flex Documents 2.xlsx match the standard headers.\n",
      "Connected to SSMS successfully.\n",
      "Query executed successfully.\n",
      "Connection to SSMS closed.\n",
      "Connected to SSMS successfully.\n",
      "Query executed successfully.\n",
      "Connection to SSMS closed.\n",
      "Duplicate Validations: Passed\n",
      "Check column 'CarrierId' for blanks or null values: Passed\n",
      "Check column 'PlanName' for blanks or null values: Passed\n",
      "Check column 'State' for blanks or null values: Passed\n",
      "Check column 'DocumentType' for blanks or null values: Passed\n",
      "Check column 'Language' for blanks or null values: Passed\n",
      "Check column 'EffectiveBeginDate' for blanks or null values: Passed\n",
      "Check column 'DocumentUrl' for blanks or null values: Passed\n",
      "Check column 'ProductStateId' for blanks or null values: Passed\n",
      "Validations completed.....\n",
      "All validations passed....\n",
      "Connected to SSMS successfully.\n",
      "Connection to SSMS closed.\n",
      "Connected to SSMS successfully.\n",
      "Data imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# input password for SSMS\\PostgreSQL connection\n",
    "server_password = getpass.getpass(\"Enter password to connect to SSMS and PostgreSQL: \")\n",
    "\n",
    "# Paths to configuration files\n",
    "config_file_path = 'AncillaryDocumentConfig.json' \n",
    "\n",
    "# Read configuration\n",
    "carrier_config = carrierConfig(config_file_path)\n",
    "# Prepare Carrier Config List for Data Ingest\n",
    "carrier_config_list = carrier_config.get_carrier_config()  \n",
    "\n",
    "# Load carrier data to dataframe \n",
    "load_carrier_config = CarrierLoader(carrier_config_list)\n",
    "main_df, validation_restuls_df = load_carrier_config.load_file()\n",
    "\n",
    "# Get distinct CarrierId list\n",
    "carrier_id = tuple(sorted([str(id[0]) for id in carrier_config_list]))\n",
    "\n",
    "# Connect to SSMS\n",
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "ssms_connection.run_connection_manager()\n",
    "\n",
    "# Set query join dbo.AncillaryProduct & dbo.AncillaryProductState to get list of ProductStateID based on values set in parameter \"carrier_id\"\n",
    "if len(carrier_id) == 1:\n",
    "    AncillaryProductState_Q = f\"\"\"\n",
    "\t\tSELECT \n",
    "\t\t\tp.ProductId,\n",
    "\t\t\tp.CarrierId,\n",
    "\t\t\tp.BrandId,\n",
    "\t\t\tp.ProductName,\n",
    "\t\t\tps.ProductStateId,\n",
    "\t\t\tps.StateCd\n",
    "\t\tFROM dbo.AncillaryProduct p \n",
    "\t\tJOIN dbo.AncillaryProductState ps on p.ProductId = ps.ProductId\n",
    "\t\tWHERE p.CarrierId = {carrier_id[0]}\n",
    "\t\t\"\"\"\n",
    "else:\n",
    "\tAncillaryProductState_Q = f\"\"\"\n",
    "\t\tSELECT \n",
    "\t\t\tp.ProductId,\n",
    "\t\t\tp.CarrierId,\n",
    "\t\t\tp.BrandId,\n",
    "\t\t\tp.ProductName,\n",
    "\t\t\tps.ProductStateId,\n",
    "\t\t\tps.StateCd\n",
    "\t\tFROM dbo.AncillaryProduct p \n",
    "\t\tJOIN dbo.AncillaryProductState ps on p.ProductId = ps.ProductId\n",
    "\t\tWHERE p.CarrierId in {carrier_id}\n",
    "\t\t\"\"\"\n",
    "\t\n",
    "# Run Query AncillaryProductState_Q and set to Dataframe \n",
    "ancillary_product_state_df = ssms_connection.query_table(AncillaryProductState_Q)\n",
    "# Close to connection to ssms\n",
    "ssms_connection.close()\n",
    "# Rename columns in ancillary_product_state_df\n",
    "ancillary_product_state_df.rename(columns={'ProductName':'PlanName', 'StateCd':'State'},inplace=True)\n",
    "\n",
    "# Pandas left join main_df and ancillary_product_state_df on CarrierId, State and Plan Name \n",
    "ancillary_document_df = pd.merge(main_df, ancillary_product_state_df, on=['CarrierId','State','PlanName'], how='left')\n",
    "\n",
    "# Update document type in ancillary_document_df if language == Spanish beginning prefix of documentType = 'SP_' && language == English beginning prefix of documentType = 'EN_'\n",
    "def update_language_docs(row):\n",
    "    if row['Language'] == 'Spanish':\n",
    "        return 'SP_' + row['DocumentType']\n",
    "    else:\n",
    "        return row['DocumentType'] \n",
    "    \n",
    "ancillary_document_df['DocumentTypeNew'] = ancillary_document_df.apply(update_language_docs, axis=1)\n",
    "\n",
    "# Set product_state_id tuple fromm ancillary_document_df\n",
    "product_state_id = tuple(sorted([str(id) for id in ancillary_document_df['ProductStateId'].unique().tolist()]))\n",
    "\n",
    "# Load dbo.AncillaryDocuments to ssms_document_df\n",
    "# Connect to SSMS\n",
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "ssms_connection.run_connection_manager()\n",
    "\n",
    "# Query Ancillary Document table where ProductStateId in {product_state_id} and Load dbo.AncillaryDocuments to ssms_document_df\n",
    "AncillaryDocument_Q = f\"\"\"\n",
    "SELECT *\n",
    "FROM dbo.AncillaryDocument \n",
    "WHERE EffectiveEndDate is null and ProductStateId in {product_state_id}\n",
    "\"\"\"\n",
    "# Run Query AncillaryProductState_Q and set to Dataframe \n",
    "ssms_document_df = ssms_connection.query_table(AncillaryDocument_Q)\n",
    "# Close to connection to ssms\n",
    "ssms_connection.close()\n",
    "\n",
    "# Set Parameters and Run validations rules and check for data load issues for all files loaded into dataframe\n",
    "check_columns_for_dup = ['ProductStateId','CarrierId', 'CarrierName', 'BrandName','PlanName','State','DocumentType','Language','EffectiveBeginDate','fileName']\n",
    "columns_to_check_if_null = ['CarrierId','PlanName','State','DocumentType','Language','EffectiveBeginDate','DocumentUrl','ProductStateId']\n",
    "set_validations = validations(ancillary_document_df,validation_restuls_df,check_columns_for_dup,columns_to_check_if_null)\n",
    "validation_restuls_df = set_validations.run_all_validations()\n",
    "\n",
    "# Update FileLoadIndicator = 'N' in ancillary_document_df for files with issues from validation_restuls_df. \n",
    "if validation_restuls_df.size > 0:\n",
    "\t# merge results with ancillary_document_df\n",
    "\tancillary_document_df = pd.merge(ancillary_document_df, validation_restuls_df, on='fileName', how='left',suffixes=('', '_new'))\n",
    "\n",
    "\t# Update the 'FileLoadIndicator' column based on the 'FileLoadIndicator_new' column and the 'fileName' match\n",
    "\tancillary_document_df['FileLoadIndicator'] = np.where(ancillary_document_df['fileName'].isin(validation_restuls_df['fileName']), ancillary_document_df['FileLoadIndicator_new'], ancillary_document_df['FileLoadIndicator'])\n",
    "\n",
    "\t# Drop the 'FileLoadIndicator_new' & 'ErrorDescription' column.\n",
    "\tancillary_document_df.drop(columns=['FileLoadIndicator_new','ErrorDescription'], inplace=True)\n",
    "else:\n",
    "    print('All validations passed....')\n",
    "\n",
    "# Set DocumentType = DocumentTypeNew in ancillary_document_df\n",
    "ancillary_document_df['DocumentType'] = ancillary_document_df['DocumentTypeNew']\n",
    "\n",
    "# Select specfic columns where FileLoadIndicator = 'Y' in ancillary_document_df and Select specific columns to prepaare to load into ssms.\n",
    "ancillary_document_df = ancillary_document_df[ancillary_document_df['FileLoadIndicator'] == 'Y']\n",
    "ancillary_document_df = ancillary_document_df[['ProductStateId','DocumentType','EffectiveBeginDate','DocumentUrl','LoadDate']]\n",
    "\n",
    "# Update FileLoadIndicator = 'N' in ancillary_document_df for files with issues from validation_restuls_df. \n",
    "ssms_document_df = pd.merge(ssms_document_df, ancillary_document_df, on=['ProductStateId','DocumentType'], how='inner',suffixes=('', '_new'))\n",
    "\n",
    "\n",
    "# Apply EffectiveEndDate and UpdateDate to previous records with new documents.\n",
    "if ssms_document_df is not None:\n",
    "\t# Set new end dates ('EffectiveEndDate') for previous Documents 1 day prior to new documents loaded and apply udpates to dbo.AncillaryDopcument based on ProductStateId\n",
    "\tssms_document_df['EffectiveEndDate'] = pd.to_datetime(ssms_document_df['EffectiveBeginDate_new'] - pd.Timedelta(days=1)).dt.date\n",
    "\tssms_document_df['UpdateDate'] = pd.Timestamp.now()\n",
    "\n",
    "\t# Convert columns to appropriate types\n",
    "\tssms_document_df['ProductStateId'] = ssms_document_df['ProductStateId'].astype(int)\n",
    "\tssms_document_df['EffectiveEndDate'] = pd.to_datetime(ssms_document_df['EffectiveEndDate'])\n",
    "\tssms_document_df['UpdateDate'] = pd.to_datetime(ssms_document_df['UpdateDate'])\n",
    "      \n",
    "\t# Set Parameters to update dbo.AncillaryDocument in ssms and postgresql\n",
    "\ttable_name = \"dbo.AncillaryDocument\"\n",
    "\tkey_columns = ['ProductStateId']  # Column(s) used as keys for the update\n",
    "\tupdate_columns = ['EffectiveEndDate', 'UpdateDate']  # Columns to update\n",
    "\n",
    "\t# Connect to SSMS\n",
    "\tssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "\tssms_connection.run_connection_manager()\n",
    "\n",
    "\t# udpate EffectiveEndDate and UpdateDate in dbo.AncillaryDocument\n",
    "\tssms_connection.update_table_with_dataframe(ssms_document_df, table_name, key_columns, update_columns)\n",
    "\n",
    "\t# Close to connection to ssms\n",
    "\tssms_connection.close()\n",
    "else:\n",
    "\tprint('Empty')\n",
    "\n",
    "# Connects to SSMS and Insert new data into SSMS dbo.AncillaryDocument where FileLoadIndicator = 'Y' from ancillary_document_df. On complete closes connection to SSMS\n",
    "table_name = 'dbo.AncillaryDocument'\n",
    "columns_to_insert = ['ProductStateId', 'DocumentType', 'EffectiveBeginDate', 'DocumentUrl','LoadDate']\n",
    "\n",
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "ssms_connection.run_connection_manager()\n",
    "ssms_connection.insert_into_sql(ancillary_document_df,table_name,columns_to_insert)\n",
    "\n",
    "# Update QA with changes applied to caliload dbo.AncillaryDocument Table to PostgreSQL \n",
    "\n",
    "# Migrate to QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SSMS successfully.\n",
      "Query executed successfully.\n",
      "Connection to SSMS closed.\n"
     ]
    }
   ],
   "source": [
    "# Load dbo.AncillaryDocuments to ssms_document_df\n",
    "# Connect to SSMS\n",
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "ssms_connection.run_connection_manager()\n",
    "\n",
    "# Query Ancillary Document table where ProductStateId in {product_state_id} and Load dbo.AncillaryDocuments to ssms_document_df\n",
    "AncillaryDocument_Q = f\"\"\"\n",
    "SELECT *\n",
    "FROM dbo.AncillaryDocument \n",
    "WHERE EffectiveEndDate is null and ProductStateId in {product_state_id}\n",
    "\"\"\"\n",
    "# Run Query AncillaryProductState_Q and set to Dataframe \n",
    "ssms_document_df = ssms_connection.query_table(AncillaryDocument_Q)\n",
    "# Close to connection to ssms\n",
    "ssms_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update FileLoadIndicator = 'N' in ancillary_document_df for files with issues from validation_restuls_df. \n",
    "ssms_document_df = pd.merge(ssms_document_df, ancillary_document_df, on=['ProductStateId','DocumentType'], how='inner',suffixes=('', '_new'))\n",
    "\n",
    "# Set new end dates ('EffectiveEndDate') for previous Documents 1 day prior to new documents loaded and apply udpates to dbo.AncillaryDopcument based on ProductStateId\n",
    "ssms_document_df['EffectiveEndDate'] = pd.to_datetime(ssms_document_df['EffectiveBeginDate_new'] - pd.Timedelta(days=1)).dt.date\n",
    "ssms_document_df['UpdateDate'] = pd.Timestamp.now()\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "ssms_document_df['ProductStateId'] = ssms_document_df['ProductStateId'].astype(int)\n",
    "ssms_document_df['EffectiveEndDate'] = pd.to_datetime(ssms_document_df['EffectiveEndDate'])\n",
    "ssms_document_df['UpdateDate'] = pd.to_datetime(ssms_document_df['UpdateDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AncillaryDocumentId</th>\n",
       "      <th>ProductStateId</th>\n",
       "      <th>DocumentType</th>\n",
       "      <th>EffectiveBeginDate</th>\n",
       "      <th>EffectiveEndDate</th>\n",
       "      <th>DocumentUrl</th>\n",
       "      <th>UpdateDate</th>\n",
       "      <th>LoadDate</th>\n",
       "      <th>EffectiveBeginDate_new</th>\n",
       "      <th>DocumentUrl_new</th>\n",
       "      <th>LoadDate_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>569</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:23:17.327404</td>\n",
       "      <td>2024-06-10 22:17:37.460</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:17:37.461492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>570</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:23:17.327404</td>\n",
       "      <td>2024-06-10 22:17:37.460</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:17:37.461492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>571</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:23:17.327404</td>\n",
       "      <td>2024-06-10 22:17:37.460</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:17:37.461492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>580</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:23:17.327404</td>\n",
       "      <td>2024-06-10 22:17:37.460</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:17:37.461492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>585</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:23:17.327404</td>\n",
       "      <td>2024-06-10 22:17:37.460</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-10 22:17:37.461492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AncillaryDocumentId  ProductStateId DocumentType EffectiveBeginDate  \\\n",
       "0                   50             569     Brochure         2024-07-01   \n",
       "1                   51             570     Brochure         2024-07-01   \n",
       "2                   52             571     Brochure         2024-06-01   \n",
       "3                   53             580     Brochure         2024-06-15   \n",
       "4                   54             585     Brochure         2024-08-01   \n",
       "\n",
       "  EffectiveEndDate                                        DocumentUrl  \\\n",
       "0       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2       2024-05-31  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3       2024-06-14  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4       2024-07-31  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "                  UpdateDate                LoadDate EffectiveBeginDate_new  \\\n",
       "0 2024-06-10 22:23:17.327404 2024-06-10 22:17:37.460             2024-07-01   \n",
       "1 2024-06-10 22:23:17.327404 2024-06-10 22:17:37.460             2024-07-01   \n",
       "2 2024-06-10 22:23:17.327404 2024-06-10 22:17:37.460             2024-06-01   \n",
       "3 2024-06-10 22:23:17.327404 2024-06-10 22:17:37.460             2024-06-15   \n",
       "4 2024-06-10 22:23:17.327404 2024-06-10 22:17:37.460             2024-08-01   \n",
       "\n",
       "                                     DocumentUrl_new  \\\n",
       "0  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "                LoadDate_new  \n",
       "0 2024-06-10 22:17:37.461492  \n",
       "1 2024-06-10 22:17:37.461492  \n",
       "2 2024-06-10 22:17:37.461492  \n",
       "3 2024-06-10 22:17:37.461492  \n",
       "4 2024-06-10 22:17:37.461492  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ssms_document_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def connect_to_postgres(config):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        conn = psycopg2.connect(**params)\n",
    "        return conn\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def update_postgres_table(df, table_name, connection_string, key_column):\n",
    "    \"\"\"\n",
    "    Update records in a PostgreSQL table based on a DataFrame.\n",
    "\n",
    "    :param df: pandas DataFrame containing the data to update\n",
    "    :param table_name: name of the target table in PostgreSQL\n",
    "    :param connection_string: connection string to connect to the PostgreSQL database\n",
    "    :param key_column: the column name used as the unique key to identify rows\n",
    "    \"\"\"\n",
    "    # Create an engine instance\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Connect to the database\n",
    "    with engine.connect() as connection:\n",
    "        for index, row in df.iterrows():\n",
    "            # Prepare the update query\n",
    "            set_clause = ', '.join([f\"{col} = :{col}\" for col in df.columns if col != key_column])\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE {table_name}\n",
    "            SET {set_clause}\n",
    "            WHERE {key_column} = :{key_column}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute the update query with the row data\n",
    "            connection.execute(text(update_query), **row.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters to update dbo.AncillaryDocument in ssms and postgresql\n",
    "table_name = \"dbo.AncillaryDocument\"\n",
    "key_columns = ['ProductStateId']  # Column(s) used as keys for the update\n",
    "update_columns = ['EffectiveEndDate', 'UpdateDate']  # Columns to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev: pg-presentation-dev.sunfirematrix.com\n",
    "# qa: pg-presentation-qa-rw.sunfirematrix.com\n",
    "# Define your PostgreSQL connection parameters in a dictionary\n",
    "config = {\n",
    "    \"dbname\": \"planpresentation\",\n",
    "    \"user\":\"sunfiresa\",\n",
    "    \"password\":\"Ij30dIcmyam\",\n",
    "    \"host\":\"pg-presentation-dev.sunfirematrix.com\",\n",
    "    \"port\":\"5432\"\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = connect_to_postgres(config)\n",
    "\n",
    "if conn is not None:\n",
    "    # Perform bulk insert\n",
    "    bulk_insert_to_table(conn, ssms_document_df, table_name)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev: pg-presentation-dev.sunfirematrix.com\n",
    "# qa: pg-presentation-qa-rw.sunfirematrix.com\n",
    "# Define your PostgreSQL connection parameters in a dictionary\n",
    "config = {\n",
    "    \"dbname\": \"planpresentation\",\n",
    "    \"user\":\"sunfiresa\",\n",
    "    \"password\":\"Ij30dIcmyam\",\n",
    "    \"host\":\"pg-presentation-dev.sunfirematrix.com\",\n",
    "    \"port\":\"5432\"\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = connect_to_postgres(config)\n",
    "\n",
    "if conn is not None:\n",
    "    # Update the PostgreSQL table\n",
    "    update_postgres_table(ssms_document_df, table_name, conn, key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SSMS successfully.\n",
      "Query executed successfully.\n",
      "Connection to SSMS closed.\n"
     ]
    }
   ],
   "source": [
    "# Load dbo.AncillaryDocuments to ssms_document_df\n",
    "# Connect to SSMS\n",
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "ssms_connection.run_connection_manager()\n",
    "\n",
    "# Query Ancillary Document table where ProductStateId in {product_state_id} and Load dbo.AncillaryDocuments to ssms_document_df\n",
    "AncillaryDocument_Q = f\"\"\"\n",
    "SELECT *\n",
    "FROM dbo.AncillaryDocument \n",
    "WHERE CAST(LoadDate as DATE) = CAST(GETDATE() as DATE) \n",
    "\"\"\"\n",
    "# Run Query AncillaryProductState_Q and set to Dataframe \n",
    "ssms_current_document_df = ssms_connection.query_table(AncillaryDocument_Q)\n",
    "# Close to connection to ssms\n",
    "ssms_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AncillaryDocumentId</th>\n",
       "      <th>ProductStateId</th>\n",
       "      <th>DocumentType</th>\n",
       "      <th>EffectiveBeginDate</th>\n",
       "      <th>EffectiveEndDate</th>\n",
       "      <th>DocumentUrl</th>\n",
       "      <th>UpdateDate</th>\n",
       "      <th>LoadDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>569</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-10 21:41:55.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>570</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-10 21:41:55.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>571</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-10 21:41:55.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>580</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-10 21:41:55.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>585</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-10 21:41:55.997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AncillaryDocumentId  ProductStateId DocumentType EffectiveBeginDate  \\\n",
       "0                    1             569     Brochure         2024-05-01   \n",
       "1                    2             570     Brochure         2024-05-01   \n",
       "2                    3             571     Brochure         2024-05-01   \n",
       "3                    4             580     Brochure         2024-05-01   \n",
       "4                    5             585     Brochure         2024-05-01   \n",
       "\n",
       "  EffectiveEndDate                                        DocumentUrl  \\\n",
       "0             None  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1             None  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2             None  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3             None  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4             None  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "  UpdateDate                LoadDate  \n",
       "0       None 2024-06-10 21:41:55.997  \n",
       "1       None 2024-06-10 21:41:55.997  \n",
       "2       None 2024-06-10 21:41:55.997  \n",
       "3       None 2024-06-10 21:41:55.997  \n",
       "4       None 2024-06-10 21:41:55.997  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ssms_current_document_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 2 (4110948826.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[58], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    if 'Date' in i[0] and 'Date' in i[1] and matched is None:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "def check_data_if_previously_loaded (df, columns_to_check):\n",
    "    for i in columns_to_check:\n",
    "        if 'Date' in i[0] and 'Date' in i[1] and matched is None:\n",
    "            check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "            matched = check_match\n",
    "        elif 'Date' in i[0] and 'Date' in i[1] and matched is not None:\n",
    "            check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "            matched = matched & check_match\n",
    "        elif 'Date' not in i[0] and 'Date' not in i[1] and matched is None:\n",
    "            check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "            matched = check_match\n",
    "        elif 'Date' not in i[0] and 'Date' not in i[1] and matched is not None:\n",
    "            check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "            matched = matched & check_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check_if_previously_loaded = (('EffectiveBeginDate','EffectiveBeginDate_new'), ('DocumentUrl','DocumentUrl_new'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emtpy\n"
     ]
    }
   ],
   "source": [
    "if matched is None:\n",
    "    print('emtpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns_to_check_if_previously_loaded:\n",
    "    if 'Date' in i[0] and 'Date' in i[1] and matched is None:\n",
    "        check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "        matched = check_match\n",
    "    elif 'Date' in i[0] and 'Date' in i[1] and matched is not None:\n",
    "        check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "        matched = matched & check_match\n",
    "    elif 'Date' not in i[0] and 'Date' not in i[1] and matched is None:\n",
    "        check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "        matched = check_match\n",
    "    elif 'Date' not in i[0] and 'Date' not in i[1] and matched is not None:\n",
    "        check_match = pd.to_datetime(ssms_document_df[col_pair1[0]]).dt.date == pd.to_datetime(ssms_document_df[col_pair1[1]]).dt.date\n",
    "        matched = matched & check_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where both pairs of columns match:\n",
      "    AncillaryDocumentId  ProductStateId DocumentType EffectiveBeginDate  \\\n",
      "48                   49             618  EN_Brochure         2024-05-01   \n",
      "\n",
      "   EffectiveEndDate                                        DocumentUrl  \\\n",
      "48       2024-04-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
      "\n",
      "                   UpdateDate                LoadDate EffectiveBeginDate_new  \\\n",
      "48 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.803             2024-05-01   \n",
      "\n",
      "                                      DocumentUrl_new  \n",
      "48  https://cignaforbrokers.com/gasbagent/cache/fo...  \n"
     ]
    }
   ],
   "source": [
    "if matched.any():\n",
    "    print(\"Rows where both pairs of columns match:\")\n",
    "    print(ssms_document_df[matched])\n",
    "else:\n",
    "    print(\"No rows found where both pairs of columns match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of performing an action when values match\n",
    "ssms_document_df['Match_Ind'] = matched.apply(lambda x: 'Matched' if x else 'Not Matched')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AncillaryDocumentId</th>\n",
       "      <th>ProductStateId</th>\n",
       "      <th>DocumentType</th>\n",
       "      <th>EffectiveBeginDate</th>\n",
       "      <th>EffectiveEndDate</th>\n",
       "      <th>DocumentUrl</th>\n",
       "      <th>UpdateDate</th>\n",
       "      <th>LoadDate</th>\n",
       "      <th>EffectiveBeginDate_new</th>\n",
       "      <th>DocumentUrl_new</th>\n",
       "      <th>Match_Action</th>\n",
       "      <th>Match_Ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>569</td>\n",
       "      <td>EN_Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-03 21:49:00.324315</td>\n",
       "      <td>2024-06-03 16:56:40.717</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>570</td>\n",
       "      <td>EN_Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-03 21:49:00.324315</td>\n",
       "      <td>2024-06-03 16:56:40.717</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>571</td>\n",
       "      <td>EN_Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-03 21:49:00.324315</td>\n",
       "      <td>2024-06-03 16:56:40.720</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>580</td>\n",
       "      <td>EN_Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-03 21:49:00.324315</td>\n",
       "      <td>2024-06-03 16:56:40.720</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>585</td>\n",
       "      <td>EN_Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-03 21:49:00.324315</td>\n",
       "      <td>2024-06-03 16:56:40.723</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AncillaryDocumentId  ProductStateId DocumentType EffectiveBeginDate  \\\n",
       "0                    1             569  EN_Brochure         2024-05-01   \n",
       "1                    2             570  EN_Brochure         2024-05-01   \n",
       "2                    3             571  EN_Brochure         2024-05-01   \n",
       "3                    4             580  EN_Brochure         2024-05-01   \n",
       "4                    5             585  EN_Brochure         2024-05-01   \n",
       "\n",
       "  EffectiveEndDate                                        DocumentUrl  \\\n",
       "0       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "                  UpdateDate                LoadDate EffectiveBeginDate_new  \\\n",
       "0 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.717             2024-07-01   \n",
       "1 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.717             2024-07-01   \n",
       "2 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.720             2024-07-01   \n",
       "3 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.720             2024-07-01   \n",
       "4 2024-06-03 21:49:00.324315 2024-06-03 16:56:40.723             2024-07-01   \n",
       "\n",
       "                                     DocumentUrl_new Match_Action    Match_Ind  \n",
       "0  https://cignaforbrokers.com/gasbagent/cache/fo...  Not Matched  Not Matched  \n",
       "1  https://cignaforbrokers.com/gasbagent/cache/fo...  Not Matched  Not Matched  \n",
       "2  https://cignaforbrokers.com/gasbagent/cache/fo...  Not Matched  Not Matched  \n",
       "3  https://cignaforbrokers.com/gasbagent/cache/fo...  Not Matched  Not Matched  \n",
       "4  https://cignaforbrokers.com/gasbagent/cache/fo...  Not Matched  Not Matched  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ssms_document_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ssms_document_df[ssms_document_df['Match_Ind'] == 'Matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_document_id =  results['AncillaryDocumentId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Previously Loaded, Please check AncillaryDocumentId: [49]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Previously Loaded, Please check AncillaryDocumentId: {ancillary_document_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AncillaryDocumentId</th>\n",
       "      <th>ProductStateId</th>\n",
       "      <th>DocumentType</th>\n",
       "      <th>EffectiveBeginDate</th>\n",
       "      <th>EffectiveEndDate</th>\n",
       "      <th>DocumentUrl</th>\n",
       "      <th>UpdateDate</th>\n",
       "      <th>LoadDate</th>\n",
       "      <th>EffectiveBeginDate_new</th>\n",
       "      <th>DocumentUrl_new</th>\n",
       "      <th>LoadDate_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>569</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 22:26:19.086468</td>\n",
       "      <td>2024-06-04 21:20:25.890</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 21:41:11.299148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>570</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 22:26:19.086468</td>\n",
       "      <td>2024-06-04 21:20:25.890</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 21:41:11.299148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>571</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 22:26:19.086468</td>\n",
       "      <td>2024-06-04 21:20:25.890</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 21:41:11.299148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>580</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 22:26:19.086468</td>\n",
       "      <td>2024-06-04 21:20:25.890</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 21:41:11.299148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>585</td>\n",
       "      <td>Brochure</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 22:26:19.086468</td>\n",
       "      <td>2024-06-04 21:20:25.890</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>https://cignaforbrokers.com/gasbagent/cache/fo...</td>\n",
       "      <td>2024-06-04 21:41:11.299148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AncillaryDocumentId  ProductStateId DocumentType EffectiveBeginDate  \\\n",
       "0                   89             569     Brochure         2024-05-01   \n",
       "1                   90             570     Brochure         2024-05-01   \n",
       "2                   91             571     Brochure         2024-05-01   \n",
       "3                   92             580     Brochure         2024-05-01   \n",
       "4                   93             585     Brochure         2024-05-01   \n",
       "\n",
       "  EffectiveEndDate                                        DocumentUrl  \\\n",
       "0       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1       2024-06-30  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2       2024-05-31  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3       2024-06-14  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4       2024-07-31  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "                  UpdateDate                LoadDate EffectiveBeginDate_new  \\\n",
       "0 2024-06-04 22:26:19.086468 2024-06-04 21:20:25.890             2024-07-01   \n",
       "1 2024-06-04 22:26:19.086468 2024-06-04 21:20:25.890             2024-07-01   \n",
       "2 2024-06-04 22:26:19.086468 2024-06-04 21:20:25.890             2024-06-01   \n",
       "3 2024-06-04 22:26:19.086468 2024-06-04 21:20:25.890             2024-06-15   \n",
       "4 2024-06-04 22:26:19.086468 2024-06-04 21:20:25.890             2024-08-01   \n",
       "\n",
       "                                     DocumentUrl_new  \\\n",
       "0  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "1  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "2  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "3  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "4  https://cignaforbrokers.com/gasbagent/cache/fo...   \n",
       "\n",
       "                LoadDate_new  \n",
       "0 2024-06-04 21:41:11.299148  \n",
       "1 2024-06-04 21:41:11.299148  \n",
       "2 2024-06-04 21:41:11.299148  \n",
       "3 2024-06-04 21:41:11.299148  \n",
       "4 2024-06-04 21:41:11.299148  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ssms_document_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE dbo.AncillaryDocument SET EffectiveEndDate = ?, UpdateDate = ? WHERE ProductStateId = ? [datetime.date(2024, 6, 30), Timestamp('2024-06-04 22:26:19.086468'), 569]\n",
      "UPDATE dbo.AncillaryDocument SET EffectiveEndDate = ?, UpdateDate = ? WHERE ProductStateId = ? [datetime.date(2024, 6, 30), Timestamp('2024-06-04 22:26:19.086468'), 570]\n",
      "UPDATE dbo.AncillaryDocument SET EffectiveEndDate = ?, UpdateDate = ? WHERE ProductStateId = ? [datetime.date(2024, 5, 31), Timestamp('2024-06-04 22:26:19.086468'), 571]\n",
      "UPDATE dbo.AncillaryDocument SET EffectiveEndDate = ?, UpdateDate = ? WHERE ProductStateId = ? [datetime.date(2024, 6, 14), Timestamp('2024-06-04 22:26:19.086468'), 580]\n",
      "UPDATE dbo.AncillaryDocument SET EffectiveEndDate = ?, UpdateDate = ? WHERE ProductStateId = ? [datetime.date(2024, 7, 31), Timestamp('2024-06-04 22:26:19.086468'), 585]\n"
     ]
    }
   ],
   "source": [
    "key_columns = ['ProductStateId']  # Column(s) used as keys for the update\n",
    "update_columns = ['EffectiveEndDate', 'UpdateDate']  # Columns to update\n",
    "\n",
    "for index, row in ssms_document_df.iterrows():\n",
    "    set_clause = ', '.join([f\"{col} = ?\" for col in update_columns])\n",
    "    where_clause = ' AND '.join([f\"{col} = ?\" for col in key_columns])\n",
    "    sql_query = f\"UPDATE {table_name} SET {set_clause} WHERE {where_clause}\"\n",
    "    parameters = [row[col] for col in update_columns] + [row[col] for col in key_columns]\n",
    "    print(sql_query, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_columns ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 00:00:00\n",
      "2024-06-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "for col in update_columns.values():\n",
    "    print(row[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'join_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m ssms_document_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m((row[\u001b[43mjoin_column\u001b[49m],)) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'join_column' is not defined"
     ]
    }
   ],
   "source": [
    "for _, row in ssms_document_df.iterrows():\n",
    "    print((row[join_column],)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples for the values to be updated\n",
    "update_values = [tuple(row[col] for col in update_columns.values()) + (row[join_column],) for _, row in ssms_document_df.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 1), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 2), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 3), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 4), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 5), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 6), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 7), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 8), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 9), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 10), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 11), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 12), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 13), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 14), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 15), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 16), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 17), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 18), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 19), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 20), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 21), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 22), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 23), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 24), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 25), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 26), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 27), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 28), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 29), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 30), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 31), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 32), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 33), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 34), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 35), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 36), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 37), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 38), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 39), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 40), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 41), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 42), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 43), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 44), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 45), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 46), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 47), (Timestamp('2024-05-31 00:00:00'), Timestamp('2024-06-03 00:00:00'), 48), (Timestamp('2024-05-31 00:00:00'), Timestamp('2024-06-03 00:00:00'), 49), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 51), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 52), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 53), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 54), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 55), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 56), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 57), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 58), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 59), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 60), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 61), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 62), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 63), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 64), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 65), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 66), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 67), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 68), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 69), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 70), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 71), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 72), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 73), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 74), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 75), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 76), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 77), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 78), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 79), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 80), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 81), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 82), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 83), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 84), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 85), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 86), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 87), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 88), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 89), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 90), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 91), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 92), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 93), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 94), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 95), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 96), (Timestamp('2024-06-30 00:00:00'), Timestamp('2024-06-03 00:00:00'), 97), (Timestamp('2024-05-31 00:00:00'), Timestamp('2024-06-03 00:00:00'), 98), (Timestamp('2024-05-31 00:00:00'), Timestamp('2024-06-03 00:00:00'), 99)]\n"
     ]
    }
   ],
   "source": [
    "print(update_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SSMS successfully.\n"
     ]
    }
   ],
   "source": [
    "ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "conn = ssms_connection.run_connection_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.executemany(sql_update_query, update_values)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 13, 'AL', None)\n",
      "(630, 13, 'AR', None)\n",
      "(631, 13, 'AZ', None)\n",
      "(632, 13, 'CO', None)\n",
      "(633, 13, 'CT', None)\n",
      "(634, 13, 'DE', None)\n",
      "(635, 13, 'FL', None)\n",
      "(636, 13, 'GA', None)\n",
      "(637, 13, 'IA', None)\n",
      "(638, 13, 'ID', None)\n",
      "(639, 13, 'IL', None)\n",
      "(640, 13, 'IN', None)\n",
      "(641, 13, 'KS', None)\n",
      "(642, 13, 'KY', None)\n",
      "(643, 13, 'LA', None)\n",
      "(644, 13, 'MA', None)\n",
      "(645, 13, 'MD', None)\n",
      "(646, 13, 'MI', None)\n",
      "(647, 13, 'MN', None)\n",
      "(648, 13, 'MO', None)\n",
      "(649, 13, 'MS', None)\n",
      "(650, 13, 'MT', None)\n",
      "(651, 13, 'NC', None)\n",
      "(652, 13, 'ND', None)\n",
      "(653, 13, 'NE', None)\n",
      "(654, 13, 'NH', None)\n",
      "(656, 13, 'NM', None)\n",
      "(657, 13, 'NV', None)\n",
      "(658, 13, 'OH', None)\n",
      "(659, 13, 'OK', None)\n",
      "(660, 13, 'OR', None)\n",
      "(661, 13, 'PA', None)\n",
      "(662, 13, 'RI', None)\n",
      "(663, 13, 'SC', None)\n",
      "(664, 13, 'SD', None)\n",
      "(665, 13, 'TN', None)\n",
      "(666, 13, 'TX', None)\n",
      "(667, 13, 'UT', None)\n",
      "(668, 13, 'VA', None)\n",
      "(669, 13, 'VT', None)\n",
      "(302, 6, 'AK', None)\n",
      "(303, 6, 'AL', None)\n",
      "(304, 6, 'AR', None)\n",
      "(305, 6, 'AZ', None)\n",
      "(306, 6, 'CO', None)\n",
      "(307, 6, 'DC', None)\n",
      "(308, 6, 'DE', None)\n",
      "(309, 6, 'FL', None)\n",
      "(310, 6, 'GA', None)\n",
      "(311, 6, 'HI', None)\n",
      "(312, 6, 'IA', None)\n",
      "(314, 6, 'IL', None)\n",
      "(315, 6, 'IN', None)\n",
      "(316, 6, 'KS', None)\n",
      "(317, 6, 'KY', None)\n",
      "(318, 6, 'LA', None)\n",
      "(319, 6, 'MA', None)\n",
      "(320, 6, 'MD', None)\n",
      "(321, 6, 'ME', None)\n",
      "(322, 6, 'MI', None)\n",
      "(323, 6, 'MN', None)\n",
      "(324, 6, 'MO', None)\n",
      "(325, 6, 'MS', None)\n",
      "(326, 6, 'MT', None)\n",
      "(327, 6, 'NC', None)\n",
      "(328, 6, 'ND', None)\n",
      "(329, 6, 'NE', None)\n",
      "(330, 6, 'NJ', None)\n",
      "(331, 6, 'NM', None)\n",
      "(332, 6, 'NV', None)\n",
      "(333, 6, 'OH', None)\n",
      "(334, 6, 'OK', None)\n",
      "(335, 6, 'OR', None)\n",
      "(336, 6, 'PA', None)\n",
      "(337, 6, 'RI', None)\n",
      "(338, 6, 'SC', None)\n",
      "(339, 6, 'SD', None)\n",
      "(340, 6, 'TN', None)\n",
      "(341, 6, 'TX', None)\n",
      "(342, 6, 'UT', None)\n",
      "(343, 6, 'VA', None)\n",
      "(344, 6, 'VT', None)\n",
      "(345, 6, 'WA', None)\n",
      "(346, 6, 'WI', None)\n",
      "(347, 6, 'WV', None)\n",
      "(348, 6, 'WY', None)\n",
      "(670, 13, 'WA', None)\n",
      "(671, 13, 'WI', None)\n",
      "(672, 13, 'WV', None)\n",
      "(673, 13, 'WY', None)\n",
      "(494, 10, 'AL', None)\n",
      "(495, 10, 'ME', None)\n",
      "(496, 10, 'SD', None)\n",
      "(497, 10, 'AK', None)\n",
      "(498, 10, 'AR', None)\n",
      "(499, 10, 'NM', None)\n",
      "(500, 10, 'AZ', None)\n",
      "(501, 10, 'CT', None)\n",
      "(502, 10, 'ND', None)\n",
      "(503, 10, 'NE', None)\n",
      "(504, 10, 'OR', None)\n",
      "(505, 10, 'UT', None)\n",
      "(506, 10, 'VA', None)\n",
      "(507, 10, 'DE', None)\n",
      "(508, 10, 'IN', None)\n",
      "(509, 10, 'WA', None)\n",
      "(510, 10, 'FL', None)\n",
      "(511, 10, 'MD', None)\n",
      "(512, 10, 'MN', None)\n",
      "(513, 10, 'NV', None)\n",
      "(514, 10, 'CO', None)\n",
      "(515, 10, 'LA', None)\n",
      "(516, 10, 'MS', None)\n",
      "(517, 10, 'MT', None)\n",
      "(518, 10, 'IA', None)\n",
      "(519, 10, 'IL', None)\n",
      "(520, 10, 'MO', None)\n",
      "(521, 10, 'KY', None)\n",
      "(522, 10, 'OH', None)\n",
      "(523, 10, 'OK', None)\n",
      "(524, 10, 'CA', None)\n",
      "(525, 10, 'MI', None)\n",
      "(526, 10, 'SC', None)\n",
      "(527, 10, 'KS', None)\n",
      "(528, 10, 'PA', None)\n",
      "(529, 10, 'DC', None)\n",
      "(530, 10, 'TX', None)\n",
      "(531, 10, 'WY', None)\n",
      "(532, 10, 'GA', None)\n",
      "(533, 10, 'ID', None)\n",
      "(534, 10, 'NC', None)\n",
      "(535, 10, 'NH', None)\n",
      "(536, 10, 'WV', None)\n",
      "(537, 10, 'TN', None)\n",
      "(538, 10, 'WI', None)\n",
      "(539, 11, 'TN', None)\n",
      "(541, 11, 'KS', None)\n",
      "(542, 11, 'UT', None)\n",
      "(543, 11, 'VA', None)\n",
      "(544, 11, 'WY', None)\n",
      "(545, 11, 'NC', None)\n",
      "(546, 11, 'NH', None)\n",
      "(547, 11, 'PA', None)\n",
      "(548, 11, 'SC', None)\n",
      "(549, 11, 'ME', None)\n",
      "(550, 11, 'ID', None)\n",
      "(551, 11, 'CO', None)\n",
      "(552, 11, 'IA', None)\n",
      "(553, 11, 'SD', None)\n",
      "(554, 11, 'WI', None)\n",
      "(555, 11, 'MO', None)\n",
      "(556, 11, 'TX', None)\n",
      "(557, 11, 'FL', None)\n",
      "(558, 11, 'IL', None)\n",
      "(559, 11, 'AL', None)\n",
      "(560, 11, 'LA', None)\n",
      "(561, 11, 'MS', None)\n",
      "(562, 11, 'MT', None)\n",
      "(563, 11, 'WV', None)\n",
      "(564, 11, 'AK', None)\n",
      "(565, 11, 'AR', None)\n",
      "(566, 11, 'KY', None)\n",
      "(567, 11, 'OH', None)\n",
      "(568, 11, 'OK', None)\n",
      "(569, 11, 'AZ', None)\n",
      "(570, 11, 'MD', None)\n",
      "(571, 11, 'MN', None)\n",
      "(572, 11, 'NV', None)\n",
      "(573, 11, 'GA', None)\n",
      "(574, 11, 'ND', None)\n",
      "(575, 11, 'NE', None)\n",
      "(576, 11, 'OR', None)\n",
      "(577, 11, 'WA', None)\n",
      "(578, 11, 'DC', None)\n",
      "(579, 11, 'NM', None)\n",
      "(580, 11, 'DE', None)\n",
      "(581, 11, 'IN', None)\n",
      "(582, 11, 'MI', None)\n",
      "(583, 11, 'CT', None)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_name=\"planpresentation\",\n",
    "db_user=\"sunfiresa\",\n",
    "db_password=\"Ij30dIcmyam\",\n",
    "db_host=\"pg-presentation-qa.sunfirematrix.com\",\n",
    "db_port=\"5432\"\n",
    "\n",
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"planpresentation\",\n",
    "    user=\"sunfiresa\",\n",
    "    password=\"Ij30dIcmyam\",\n",
    "    host=\"pg-presentation-qa-rw.sunfirematrix.com\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a connection string\n",
    "conn_str = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM dbo.ancillaryproductstate\")\n",
    "\n",
    "# Fetch the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects to SSMS and Insert new data into SSMS dbo.AncillaryDocument where FileLoadIndicator = 'Y' from ancillary_document_df. On complete closes connection to SSMS\n",
    "table_name = 'dbo.ancillarydocument'\n",
    "columns_to_insert = ['ProductStateId', 'DocumentType', 'EffectiveBeginDate', 'DocumentUrl','LoadDate']\n",
    "\n",
    "#ssms_connection = SSMSConnnectionManager(carrier_config.read_config(),server_password)\n",
    "#ssms_connection.run_connection_manager()\n",
    "#ssms_connection.insert_into_sql(ancillary_document_df,table_name,columns_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def connect_to_postgres(config):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        conn = psycopg2.connect(**params)\n",
    "        return conn\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def bulk_insert_to_table(conn, df, table_name):\n",
    "    \"\"\" Bulk insert dataframe into table \"\"\"\n",
    "    # Create a list of tuples from the dataframe values\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(list(df.columns))\n",
    "\n",
    "    # SQL query to execute\n",
    "    query = f\"INSERT INTO {table_name}({cols}) VALUES %s\"\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"Bulk insert complete\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk insert complete\n"
     ]
    }
   ],
   "source": [
    "# dev: pg-presentation-dev.sunfirematrix.com\n",
    "# qa: pg-presentation-qa-rw.sunfirematrix.com\n",
    "# Define your PostgreSQL connection parameters in a dictionary\n",
    "config = {\n",
    "    \"dbname\": \"planpresentation\",\n",
    "    \"user\":\"sunfiresa\",\n",
    "    \"password\":\"Ij30dIcmyam\",\n",
    "    \"host\":\"pg-presentation-dev.sunfirematrix.com\",\n",
    "    \"port\":\"5432\"\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = connect_to_postgres(config)\n",
    "\n",
    "if conn is not None:\n",
    "    # Perform bulk insert\n",
    "    bulk_insert_to_table(conn, ssms_current_document_df, table_name)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
